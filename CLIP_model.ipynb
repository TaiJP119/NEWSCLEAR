{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb61fa34-8ec4-43e8-a7d9-70bddd4a434b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DKpro\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f729177-9a76-4ad7-bc00-c956036d0c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Fake/Fake.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14be97fe-4653-4b70-9fbd-5459fcc026a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23476</th>\n",
       "      <td>McPain: John McCain Furious That Iran Treated ...</td>\n",
       "      <td>21st Century Wire says As 21WIRE reported earl...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>January 16, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23477</th>\n",
       "      <td>JUSTICE? Yahoo Settles E-mail Privacy Class-ac...</td>\n",
       "      <td>21st Century Wire says It s a familiar theme. ...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>January 16, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23478</th>\n",
       "      <td>Sunnistan: US and Allied ‘Safe Zone’ Plan to T...</td>\n",
       "      <td>Patrick Henningsen  21st Century WireRemember ...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>January 15, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23479</th>\n",
       "      <td>How to Blow $700 Million: Al Jazeera America F...</td>\n",
       "      <td>21st Century Wire says Al Jazeera America will...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>January 14, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23480</th>\n",
       "      <td>10 U.S. Navy Sailors Held by Iranian Military ...</td>\n",
       "      <td>21st Century Wire says As 21WIRE predicted in ...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>January 12, 2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23481 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0       Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1       Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2       Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3       Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4       Pope Francis Just Called Out Donald Trump Dur...   \n",
       "...                                                  ...   \n",
       "23476  McPain: John McCain Furious That Iran Treated ...   \n",
       "23477  JUSTICE? Yahoo Settles E-mail Privacy Class-ac...   \n",
       "23478  Sunnistan: US and Allied ‘Safe Zone’ Plan to T...   \n",
       "23479  How to Blow $700 Million: Al Jazeera America F...   \n",
       "23480  10 U.S. Navy Sailors Held by Iranian Military ...   \n",
       "\n",
       "                                                    text      subject  \\\n",
       "0      Donald Trump just couldn t wish all Americans ...         News   \n",
       "1      House Intelligence Committee Chairman Devin Nu...         News   \n",
       "2      On Friday, it was revealed that former Milwauk...         News   \n",
       "3      On Christmas day, Donald Trump announced that ...         News   \n",
       "4      Pope Francis used his annual Christmas Day mes...         News   \n",
       "...                                                  ...          ...   \n",
       "23476  21st Century Wire says As 21WIRE reported earl...  Middle-east   \n",
       "23477  21st Century Wire says It s a familiar theme. ...  Middle-east   \n",
       "23478  Patrick Henningsen  21st Century WireRemember ...  Middle-east   \n",
       "23479  21st Century Wire says Al Jazeera America will...  Middle-east   \n",
       "23480  21st Century Wire says As 21WIRE predicted in ...  Middle-east   \n",
       "\n",
       "                    date  \n",
       "0      December 31, 2017  \n",
       "1      December 31, 2017  \n",
       "2      December 30, 2017  \n",
       "3      December 29, 2017  \n",
       "4      December 25, 2017  \n",
       "...                  ...  \n",
       "23476   January 16, 2016  \n",
       "23477   January 16, 2016  \n",
       "23478   January 15, 2016  \n",
       "23479   January 14, 2016  \n",
       "23480   January 12, 2016  \n",
       "\n",
       "[23481 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36087008-a2ff-4d76-80b8-bed867bbd89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIPTextClassifier(\n",
      "  (clip): CLIPModel(\n",
      "    (text_model): CLIPTextTransformer(\n",
      "      (embeddings): CLIPTextEmbeddings(\n",
      "        (token_embedding): Embedding(49408, 512)\n",
      "        (position_embedding): Embedding(77, 512)\n",
      "      )\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-11): 12 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPSdpaAttention(\n",
      "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
      "        (position_embedding): Embedding(50, 768)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-11): 12 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPSdpaAttention(\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
      "    (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CLIPTextClassifier()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30618d54-333c-4fd9-861a-fca81baddda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPModel, CLIPTokenizer\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class CLIPTextClassifier(nn.Module):\n",
    "    def __init__(self, pretrained=\"openai/clip-vit-base-patch32\"):\n",
    "        super().__init__()\n",
    "        self.clip = CLIPModel.from_pretrained(pretrained)\n",
    "\n",
    "        for p in self.clip.vision_model.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.clip.visual_projection.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        hidden = self.clip.config.projection_dim\n",
    "        self.classifier = nn.Linear(hidden, 1)\n",
    "\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(pretrained)\n",
    "\n",
    "    def forward(self, texts):\n",
    "        enc = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=77,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.classifier.weight.device)\n",
    "\n",
    "        text_embeds = self.clip.get_text_features(\n",
    "            input_ids=enc[\"input_ids\"],\n",
    "            attention_mask=enc[\"attention_mask\"]\n",
    "        )\n",
    "\n",
    "        logits = self.classifier(text_embeds)\n",
    "        return logits.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a52156b-e041-4695-8e61-5be5e63915c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def load_imdb_data(fake_csv: str, true_csv: str):\n",
    "    fk = pd.read_csv(fake_csv)\n",
    "    tr = pd.read_csv(true_csv)\n",
    "    texts  = fk[\"text\"].tolist() + tr[\"text\"].tolist()\n",
    "    labels = [0]*len(fk) + [1]*len(tr)\n",
    "    return texts, labels\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts, self.labels = texts, labels\n",
    "    def __len__(self):            \n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):   \n",
    "        return self.texts[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c898cc60-a877-4b7e-b99e-23b2efde4b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|███████████████████████████████████████████████████████████████████| 2245/2245 [10:05<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] train_loss=0.0082  val_acc=99.900%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|███████████████████████████████████████████████████████████████████| 2245/2245 [09:40<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] train_loss=0.0053  val_acc=99.944%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|███████████████████████████████████████████████████████████████████| 2245/2245 [09:39<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] train_loss=0.0019  val_acc=100.000%\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "texts, labels = load_imdb_data(\"Fake/Fake.csv\", \"True/True.csv\")\n",
    "tr_x, val_x, tr_y, val_y = train_test_split(texts, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "BATCH = 16\n",
    "train_dl = DataLoader(NewsDataset(tr_x, tr_y), batch_size=BATCH, shuffle=True)\n",
    "val_dl   = DataLoader(NewsDataset(val_x, val_y), batch_size=BATCH)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model  = CLIPTextClassifier().to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),lr=2e-5)\n",
    "\n",
    "EPOCHS = 3\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for batch_texts, batch_labels in tqdm(train_dl, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        batch_labels = batch_labels.float().to(device)\n",
    "        logits = model(batch_texts)\n",
    "        loss   = criterion(logits, batch_labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * len(batch_labels)\n",
    "\n",
    "    avg_loss = running_loss / len(train_dl.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_texts, batch_labels in val_dl:\n",
    "            batch_labels = batch_labels.float().to(device)\n",
    "            logits = model(batch_texts)\n",
    "            preds  = (torch.sigmoid(logits) > 0.5).long()\n",
    "            correct += (preds.cpu() == batch_labels.cpu()).sum().item()\n",
    "            total   += len(batch_labels)\n",
    "    acc = correct / total\n",
    "    print(f\"[Epoch {epoch+1}] train_loss={avg_loss:.4f}  val_acc={acc:.3%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e52046d9-629f-4f46-a173-87a9f29f7ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./clip_text_fake_news\\\\tokenizer_config.json',\n",
       " './clip_text_fake_news\\\\special_tokens_map.json',\n",
       " './clip_text_fake_news\\\\vocab.json',\n",
       " './clip_text_fake_news\\\\merges.txt',\n",
       " './clip_text_fake_news\\\\added_tokens.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, torch\n",
    "\n",
    "SAVE_DIR = \"./clip_text_fake_news\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "torch.save(\n",
    "    {\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"epoch\": epoch + 1,\n",
    "    },\n",
    "    os.path.join(SAVE_DIR, \"pytorch_model.bin\")\n",
    ")\n",
    "\n",
    "model.tokenizer.save_pretrained(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3588778-044d-4b79-8d1e-4465615e9b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DKpro\\AppData\\Local\\Temp\\ipykernel_21088\\2959785651.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sd = torch.load(os.path.join(ckpt_dir, \"pytorch_model.bin\"), map_location=device)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPTokenizer\n",
    "\n",
    "def load_local_model(ckpt_dir, device=\"cpu\"):\n",
    "    model = CLIPTextClassifier()\n",
    "    sd = torch.load(os.path.join(ckpt_dir, \"pytorch_model.bin\"), map_location=device)\n",
    "    model.load_state_dict(sd[\"model_state\"])\n",
    "    model.to(device).eval()\n",
    "\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(ckpt_dir)\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_local_model(\"./clip_text_fake_news\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97c721e6-bab7-4e47-bc8a-8e6f67816b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true  (confidence=0.999)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "def predict(text, model=model, device=device):\n",
    "    with torch.no_grad():\n",
    "        logits = model([text])\n",
    "        prob   = torch.sigmoid(logits)[0].item()\n",
    "\n",
    "    label = \"true\" if prob >= 0.5 else \"fake\"\n",
    "    return label, prob\n",
    "\n",
    "sample = (\n",
    "\"\"\"From the moment a Norfolk Southern ‘bomb train’ derailed in East Palestine, OH, on February 3, 2023, traumatized and chemically exposed residents became another political football to be kicked around by Republicans, Democrats, and the media. Nearly two years since the avoidable catastrophe that changed their lives forever, residents in and around East Palestine and their families have been left to live in a toxic “sacrifice zone.” Like in 2020, the majority of voters in this part of Ohio and Pennsylvania will likely vote for Donald Trump in 2024, though plenty have given up on the whole system. In this on-the-ground documentary report, TRNN Editor-in-Chief Maximillian Alvarez and Steve Mellon from the Pittsburgh Union Progress go to East Palestine to speak with residents face to face, deep in the heart of so-called “Trump Country,” and what they find is a stark reminder that working-class communities have way more in common than corporate media and corporate politicians want us to believe.\"\"\"\n",
    ")\n",
    "\n",
    "label, conf = predict(sample)\n",
    "print(f\"{label}  (confidence={conf:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428bc6d6-3fbb-437f-9210-4a34e08f593e",
   "metadata": {},
   "source": [
    "# Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2f9deef-302a-43c4-b2f4-43ac3066c54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train  loss=0.2124  acc=91.16%\n",
      "  val    loss=0.0740  acc=97.63%\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train  loss=0.0567  acc=97.81%\n",
      "  val    loss=0.0995  acc=96.66%\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train  loss=0.0371  acc=98.71%\n",
      "  val    loss=0.0661  acc=97.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import os, glob, random\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from transformers import CLIPModel, CLIPImageProcessor\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "class FrameDataset(Dataset):\n",
    "    IMG_EXT = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\", \".webp\"}\n",
    "\n",
    "    def __init__(self, root: str):\n",
    "        self.paths, self.labels = [], []\n",
    "        for lbl, sub in enumerate([\"Real\", \"Fake\"]):\n",
    "            folder = os.path.join(root, sub)\n",
    "            for p in glob.glob(os.path.join(folder, \"*\")):\n",
    "                if os.path.splitext(p)[1].lower() in self.IMG_EXT:\n",
    "                    self.paths.append(p)\n",
    "                    self.labels.append(lbl)\n",
    "\n",
    "    def __len__(self): return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
    "        return img, self.labels[idx]\n",
    "\n",
    "\n",
    "def collate_pil(batch):\n",
    "    imgs, labels = zip(*batch)\n",
    "    return list(imgs), torch.tensor(labels)\n",
    "\n",
    "class CLIPVisionClassifier(nn.Module):\n",
    "    def __init__(self, ckpt=\"openai/clip-vit-base-patch32\"):\n",
    "        super().__init__()\n",
    "        self.clip = CLIPModel.from_pretrained(ckpt)\n",
    "\n",
    "        # freeze everything except vision encoder → classifier\n",
    "        self.clip.text_model.requires_grad_(False)\n",
    "        self.clip.text_projection.requires_grad = False\n",
    "        self.clip.logit_scale.requires_grad = False\n",
    "\n",
    "        hid = self.clip.config.projection_dim          # 512\n",
    "        self.classifier = nn.Linear(hid, 1)            # binary logit\n",
    "        self.processor = CLIPImageProcessor.from_pretrained(ckpt)\n",
    "\n",
    "    def _encode(self, pil_list):\n",
    "        batch = self.processor(images=pil_list,\n",
    "                               return_tensors=\"pt\").to(self.classifier.weight.device)\n",
    "        return self.clip.get_image_features(pixel_values=batch.pixel_values)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = x if isinstance(x, torch.Tensor) else self._encode(x)\n",
    "        return self.classifier(feats).squeeze(1)\n",
    "        \n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    loss_sum = correct = n = 0\n",
    "    for imgs, y in tqdm(loader, leave=False):\n",
    "        y = y.float().to(device)\n",
    "        logits = model(imgs)\n",
    "        loss_sum += criterion(logits, y).item() * y.size(0)\n",
    "\n",
    "        preds = (torch.sigmoid(logits) > 0.5).long()\n",
    "        correct += (preds == y.long()).sum().item()\n",
    "        n += y.size(0)\n",
    "    return loss_sum / n, correct / n\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimiser, device):\n",
    "    model.train()\n",
    "    loss_sum = correct = n = 0\n",
    "    for imgs, y in tqdm(loader, leave=False):\n",
    "        y = y.float().to(device)\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        loss_sum += loss.item() * y.size(0)\n",
    "        preds = (torch.sigmoid(logits) > 0.5).long()\n",
    "        correct += (preds == y.long()).sum().item()\n",
    "        n += y.size(0)\n",
    "    return loss_sum / n, correct / n\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # config\n",
    "    set_seed(42)\n",
    "    root_dir   = \"Final Dataset\"\n",
    "    batch_size = 32\n",
    "    epochs     = 3\n",
    "    lr         = 1e-5\n",
    "    device     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    full_ds = FrameDataset(root_dir)\n",
    "    idx = list(range(len(full_ds))); random.shuffle(idx)\n",
    "    split = int(0.8 * len(idx))\n",
    "    train_ds, val_ds = Subset(full_ds, idx[:split]), Subset(full_ds, idx[split:])\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
    "                          num_workers=0, collate_fn=collate_pil, pin_memory=True)\n",
    "    val_dl   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False,\n",
    "                          num_workers=0, collate_fn=collate_pil, pin_memory=True)\n",
    "\n",
    "    model = CLIPVisionClassifier().to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimiser = optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=lr, weight_decay=1e-4\n",
    "    )\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        print(f\"\\nEpoch {ep}/{epochs}\")\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_dl, criterion, optimiser, device)\n",
    "        vl_loss, vl_acc = evaluate(model, val_dl, criterion, device)\n",
    "        print(f\"  train  loss={tr_loss:.4f}  acc={tr_acc:.2%}\")\n",
    "        print(f\"  val    loss={vl_loss:.4f}  acc={vl_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15028406-7d06-451d-b138-97eaefc40283",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Image' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      5\u001b[0m test_img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Dataset/Fake/0EJSVRSK2A.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m logit \u001b[38;5;241m=\u001b[39m model([\u001b[43mtest_img\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)])\n\u001b[0;32m      7\u001b[0m prob  \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(logit)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfake\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prob \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreal\u001b[39m\u001b[38;5;124m\"\u001b[39m, prob)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Image' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"vision_deepfake.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44c199d8-8803-4e29-9e50-4449d446b8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DKpro\\AppData\\Local\\Temp\\ipykernel_7664\\3987242799.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"vision_deepfake.bin\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake 0.9992254972457886\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = CLIPVisionClassifier().to(device)\n",
    "model.load_state_dict(torch.load(\"vision_deepfake.bin\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "test_img = Image.open(\"Final Dataset/Fake/0EJSVRSK2A.jpg\").convert(\"RGB\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logit = model([test_img])\n",
    "    prob  = torch.sigmoid(logit).item()\n",
    "\n",
    "print(\"fake\" if prob > 0.5 else \"real\", prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531dd053-b71e-4e8c-a4b8-161ab4910f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
